{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DS-GA 3001.009 Modeling Time Series Data\n",
    "\n",
    "# Week 3 Kalman Filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Install PyKalman\n",
    "# pip install pykalman\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from pykalman import KalmanFilter\n",
    "from scipy.stats import multivariate_normal\n",
    "\n",
    "# Data Visualiztion\n",
    "def plot_kalman(x,y,nx,ny,kx=None,ky=None, plot_type=\"r-\", label=None, title='Parabola'):\n",
    "    \"\"\"\n",
    "    Plot the trajectory\n",
    "    \"\"\"\n",
    "    fig, ax = plt.subplots(1,2, figsize=(15,4))\n",
    "    if kx is not None and ky is not None:\n",
    "        ax[0].plot(x,y,'g-',nx,ny,'b.',kx,ky, plot_type)\n",
    "        ax[0].plot(kx[0], ky[0], 'or')\n",
    "        ax[0].plot(kx[-1], ky[-1], 'xr')\n",
    "        \n",
    "        ax[1].plot(x, kx, '.k', label='latent dim 1')\n",
    "        ax[1].plot(y, ky, '.', color='grey', label='latent dim 2')\n",
    "        ax[1].set_xlabel('real latent')\n",
    "        ax[1].set_ylabel('estimated latent')\n",
    "        ax[1].legend()\n",
    "    else:\n",
    "        ax[0].plot(x,y,'g-',nx,ny,'b.')\n",
    "        \n",
    "        ax[1].plot(x, nx, '.k', label='dim 1')\n",
    "        ax[1].plot(y, ny, '.', color='grey', label='dim 2')\n",
    "        ax[1].set_xlabel('latent')\n",
    "        ax[1].set_ylabel('observed')\n",
    "        ax[1].legend()\n",
    "\n",
    "    ax[0].set_xlabel('X position')\n",
    "    ax[0].set_ylabel('Y position')\n",
    "    ax[0].set_title(title)\n",
    "    ax[0].set_aspect(1)\n",
    "    ax[1].set_aspect(1)\n",
    "    \n",
    "    if kx is not None and ky is not None and label is not None:\n",
    "        ax[0].legend(('true','measured', label))\n",
    "    else:\n",
    "        ax[0].legend(('true','measured'))\n",
    "    \n",
    "    return fig\n",
    "    \n",
    "def visualize_line_plot(data, xlabel, ylabel, title):\n",
    "    \"\"\"\n",
    "    Function that visualizes a line plot\n",
    "    \"\"\"\n",
    "    plt.plot(data)\n",
    "    plt.xlabel(xlabel)\n",
    "    plt.ylabel(ylabel)\n",
    "    plt.title(title)\n",
    "    plt.show()\n",
    "    \n",
    "def print_parameters(kf_model, need_params=None, evals=False):\n",
    "    \"\"\"\n",
    "    Function that prints out the parameters for a Kalman Filter\n",
    "    @param - kf_model : the model object\n",
    "    @param - need_params : a list of string\n",
    "    \"\"\"\n",
    "    if evals:\n",
    "        if need_params is None:\n",
    "            need_params1 = ['transition_matrices', 'transition_covariance', 'observation_covariance', 'initial_state_covariance']\n",
    "            need_params2 = ['observation_matrices', 'initial_state_mean']\n",
    "        for param in need_params1: \n",
    "            tmp = np.linalg.eig(getattr(kf_model, param))[0]\n",
    "            print(\"{0} = {1}, shape = {2}\\n\".format(param, tmp, tmp.shape))\n",
    "        for param in need_params2: \n",
    "            print(\"{0} = {1}, shape = {2}\\n\".format(param, getattr(kf_model, param), getattr(kf_model, param).shape))\n",
    "    else:\n",
    "        if need_params is None:\n",
    "            need_params = ['transition_matrices', 'observation_matrices', 'transition_covariance', 'observation_covariance', \n",
    "                            'initial_state_mean', 'initial_state_covariance']\n",
    "        for param in need_params: \n",
    "            print(\"{0} = {1}, shape = {2}\\n\".format(param, getattr(kf_model, param), getattr(kf_model, param).shape))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Kalman"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "We want to infer the latent variable $z_n$ given the observed variable $x_n$.\n",
    "\n",
    "$$P(z_n|x_1, ..., x_n, x_{n+1}, ..., x_N)\\sim N(\\hat{\\mu_n}, \\hat{V_n})$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Forward: Filtering\n",
    "obtain estimates of latent by running the filtering from $n=0,....N$ \n",
    "\n",
    "#### prediction given latent space parameters\n",
    "<br>\n",
    "<img src='img/LDS_latent.svg', width = 110, height=90>\n",
    "\n",
    "\n",
    "$$z_n^{pred}\\sim N(\\mu_n^{pred},V_n^{pred})$$\n",
    "\n",
    "$$\\mu_n^{pred}=A\\mu_{n-1}$$\n",
    "\n",
    "*this is the prediction for $z_n$ obtained simply by taking the expected value of $z_{n-1}$ and projecting it forward one step using the transition probability matrix $A$*\n",
    "\n",
    "$$V_n^{pred}=AV_{n-1}A^T+\\Gamma$$\n",
    "*same for the covariance taking into account the noise covariance $\\Gamma$*\n",
    "\n",
    "#### correction (innovation) from observation\n",
    "<br>\n",
    "<img src='img/LDS_observed.svg', width = 40, height=80>\n",
    "\n",
    "project to observational space:\n",
    "$$x_n^{pred}\\sim N(C\\mu_n^{pred}, CV_n^{pred}C^T+\\Sigma)$$\n",
    "\n",
    "correct prediction by actual data:\n",
    "$$z_n^{innov}\\sim N(\\mu_n^{innov}, V_n^{innov})$$\n",
    "\n",
    "$$\\mu_n^{innov}=\\mu_n^{pred}+K_n(x_n-C\\mu_n^{pred})$$\n",
    "\n",
    "$$V_n^{innov}=(I-K_nC)V_n^{pred}$$\n",
    "\n",
    "Kalman gain matrix: \n",
    "$$K_n=V_n^{pred}C^T(CV_n^{pred}C^T+\\Sigma)^{-1}$$\n",
    "\n",
    "*we use the latent-only prediction to project it to the observational space and compute a correction proportional to the error $x_n-CAz_{n-1}$ between prediction and data, coefficient of this correction is the Kalman gain matrix*\n",
    "\n",
    "<br>\n",
    "<img src='img/Kfilter_Bishop.png', width = 600, height=600>\n",
    "from Bishop (2006), chapter 13.3\n",
    "\n",
    "*if measurement noise is small and dynamics are fast -> estimation will depend mostly on observed data*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Backward: Smoothing\n",
    "\n",
    "<br>\n",
    "<img src='img/LDS_smooth.svg', width = 110, height=100>\n",
    "\n",
    "obtain estimates by propagating from $x_n$ back to $x_1$ using results of forward pass ($\\mu_n^{innov}, V_n^{innov}, V_n^{pred}$)\n",
    "\n",
    "\n",
    "$$N(z_n|\\mu_n^{smooth}, V_n^{smooth})$$\n",
    "\n",
    "$$\\mu_n^{smooth}=\\mu_n^{innov}+J_n(\\mu_{n+1}^{smooth}-A\\mu_n^{innov})$$\n",
    "\n",
    "$$V_n^{smooth}=V_n^{innov}+J_n(V_{n+1}^{smooth}-V_{n+1}^{pred})J_n^T$$\n",
    "\n",
    "$$J_N=V_n^{innov}A^T (V_{n+1}^{pred})^{-1}$$\n",
    "\n",
    "This gives us the final estimate for $z_n$.\n",
    "\n",
    "$$\\hat{\\mu_n}=\\mu_n^{smooth}$$\n",
    "$$\\hat{V_n}=V_n^{smooth}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EM algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- want to maximize $log p(x|\\theta)$\n",
    "\n",
    "- need to marginalize out latent *(which is not tractable)*\n",
    "\n",
    "$$log \\left(p(x|\\theta)\\right)=log \\left(\\int p(x,z|\\theta)dz\\right)$$\n",
    "\n",
    "- add a probability distribution $q(z)$ which will approximate the latent distribution \n",
    "\n",
    "$$=\\int_z q(z) log p(x|\\theta) dz$$\n",
    "\n",
    "- can be rewritten as\n",
    "\n",
    "$$=\\mathcal{L}(q,\\theta)+KL\\left(q(z)||p(z|x),\\theta\\right)$$\n",
    "\n",
    "- $\\mathcal{L}(q,\\theta)$ contains the joint distribution of $x$ and $z$\n",
    "\n",
    "- $KL(q||p)$ contains the conditional distribution of $z|x$\n",
    "\n",
    "#### Expectation step\n",
    "- parameters are kept fixed\n",
    "- find a good approximation $q(z)$: maximize lower bound $\\mathcal{L}(q,\\theta)$ with respect to $q(z)$\n",
    "- (already implemented Kalman filter+smoother)\n",
    "\n",
    "#### Maximization step\n",
    "- keep distribution $q(z)$ fixed\n",
    "- change parameters to maximize the lower bound $\\mathcal{L}(q,\\theta)$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### M-step\n",
    "*(see Bishop, chapter 13.3.2 Learning in LDS)*\n",
    "\n",
    "Update parameters of the probability distribution\n",
    "\n",
    "Initial parameters\n",
    "$$\\mu_0^{new}=E(z_1)$$\n",
    "$$\\Gamma_0^{new} = E(z_1z_1^T)-E(z_1)E(z_1^T)$$\n",
    "\n",
    "Latent parameters\n",
    "$$A^{new} = \\left(\\sum_{n=2}^N E(z_nz_{n-1}^T)\\right)\\left(\\sum_{n=2}^N E(z_{n-1}z_{n-1}^T)\\right)^{-1}$$\n",
    "$$\\Gamma^{new} = \\frac{1}{N-1}\\sum_{n=2}^N E(z_nz_n^T)-A^{new}E(z_{n-1}z_{n}^T)-E(z_nz_{n-1}^T)A^{new}+A^{new}E(z_{n-1}z_{n-1}^T)(A^{new})^{T}$$\n",
    "\n",
    "Observable space parameters\n",
    "$$C^{new}=\\left(\\sum_{n=1}^N x_n E(z_n^T)\\right)\\left(\\sum_{n=1}^N E(z_n z_n^T)\\right)^{-1}$$\n",
    "$$\\Sigma^{new}=\\frac{1}{N}\\sum_{n=1}^Nx_nx_n^T-C^{new}E(z_n)x_n^T-x_nE(z_n^T)C^{new}+C^{new}E(z_nz_n^T)C_{new}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*For the updates in the M-step we will need the following posterior marginals obtained from the Kalman smoothing results* $\\hat{\\mu}_n, \\hat{V}_n$\n",
    "\n",
    "$$E(z_n)=\\hat{\\mu}_n$$\n",
    "$$E(z_nz_{n-1}^T)=J_{n-1}\\hat{V}_n+\\hat{\\mu}_n\\hat{\\mu}_{n-1}^T$$\n",
    "$$E(z_nz_{n}^T)=\\hat{V}_n+\\hat{\\mu}_n\\hat{\\mu}_{n}^T$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kalman + EM Implementation\n",
    "\n",
    "In this part of the exercise, you will implement the EM algorithm, building up on the exercises from last week. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class MyKalmanFilter:\n",
    "    \"\"\"\n",
    "    Class that implements the Kalman Filter\n",
    "    \"\"\"\n",
    "    def __init__(self, n_dim_state=2, n_dim_obs=2):\n",
    "        \"\"\"\n",
    "        @param n_dim_state: dimension of the laten variables\n",
    "        @param n_dim_obs: dimension of the observed variables\n",
    "        \"\"\"\n",
    "        self.n_dim_state = n_dim_state\n",
    "        self.n_dim_obs = n_dim_obs\n",
    "        self.transition_matrices = np.eye(n_dim_state)\n",
    "        self.transition_covariance = np.eye(n_dim_state)\n",
    "        self.observation_matrices = np.eye(n_dim_obs, n_dim_state)\n",
    "        self.observation_covariance = np.eye(n_dim_obs)\n",
    "        self.initial_state_mean = np.zeros(n_dim_state)\n",
    "        self.initial_state_covariance = np.eye(n_dim_state)\n",
    "        \n",
    "        \n",
    "    def sample(self, n_timesteps, initial_state=None, random_seed=None):\n",
    "        \"\"\"\n",
    "        Method that gives samples\n",
    "        @param initial_state: numpy array whose length == self.n_dim_state\n",
    "        @param random_seed: an integer, for test purpose\n",
    "        @output state: a 2d numpy array with dimension [n_timesteps, self.n_dim_state]\n",
    "        @output observation: a 2d numpy array with dimension [n_timesteps, self.n_dim_obs]\n",
    "        \"\"\"\n",
    "        \n",
    "        latent_state = np.zeros([n_timesteps, self.n_dim_state])\n",
    "        observed_state = np.zeros([n_timesteps, self.n_dim_obs])\n",
    "        \n",
    "        ################\n",
    "        ##### TODO #####\n",
    "        ################\n",
    "        # produce samples\n",
    "\n",
    "        \n",
    "        return latent_state, observed_state\n",
    "    \n",
    "        \n",
    "    def filter(self, X):\n",
    "        \"\"\"\n",
    "        Method that performs Kalman filtering\n",
    "        @param X: a numpy 2D array whose dimension is [n_example, self.n_dim_obs]\n",
    "        @output: filtered_state_means: a numpy 2D array whose dimension is [n_example, self.n_dim_state]\n",
    "        @output: filtered_state_covariances: a numpy 3D array whose dimension is [n_example, self.n_dim_state, self.n_dim_state]\n",
    "        \"\"\"\n",
    "        \n",
    "        # validate inputs\n",
    "        n_example, observed_dim = X.shape\n",
    "        assert observed_dim==self.n_dim_obs\n",
    "        \n",
    "        # create holders for outputs\n",
    "        filtered_state_means = np.zeros( [n_example, self.n_dim_state] )\n",
    "        filtered_state_covariances = np.zeros( [n_example, self.n_dim_state, self.n_dim_state] )\n",
    "        \n",
    "        #################################################################################\n",
    "        ################## insert your own filter here ##################################\n",
    "        #################################################################################\n",
    "        # below: this is an alternative if you do not have an implementation of filtering\n",
    "        kf = KalmanFilter(n_dim_state=self.n_dim_state, n_dim_obs=self.n_dim_obs)\n",
    "        need_params = ['transition_matrices', 'observation_matrices', 'transition_covariance', \n",
    "          'observation_covariance', 'initial_state_mean', 'initial_state_covariance']\n",
    "        for param in need_params:\n",
    "            setattr(kf, param, getattr(self, param))\n",
    "        filtered_state_means, filtered_state_covariances = kf.filter(X)\n",
    "        #################################################################################\n",
    "        \n",
    "        return filtered_state_means, filtered_state_covariances\n",
    "    \n",
    "    def smooth(self, X):\n",
    "        \"\"\"\n",
    "        Method that performs the Kalman Smoothing\n",
    "        @param X: a numpy 2D array whose dimension is [n_example, self.n_dim_obs]\n",
    "        @output: smoothed_state_means: a numpy 2D array whose dimension is [n_example, self.n_dim_state]\n",
    "        @output: smoothed_state_covariances: a numpy 3D array whose dimension is [n_example, self.n_dim_state, self.n_dim_state]\n",
    "        \"\"\"\n",
    "        # TODO: implement smoothing\n",
    "        \n",
    "        # validate inputs\n",
    "        n_example, observed_dim = X.shape\n",
    "        assert observed_dim==self.n_dim_obs\n",
    "        \n",
    "        # run the forward path\n",
    "        mu_list, v_list = self.filter(X)\n",
    "        \n",
    "        # create holders for outputs\n",
    "        smoothed_state_means = np.zeros( (n_example, self.n_dim_state) )\n",
    "        smoothed_state_covariances = np.zeros( (n_example, self.n_dim_state, self.n_dim_state) )\n",
    "        \n",
    "        #################################################################################\n",
    "        ################## insert your own smoother here ################################\n",
    "        #################################################################################\n",
    "        # below: this is an alternative if you do not have an implementation of smoothing\n",
    "        kf = KalmanFilter(n_dim_state=self.n_dim_state, n_dim_obs=self.n_dim_obs)\n",
    "        need_params = ['transition_matrices', 'observation_matrices', 'transition_covariance', \n",
    "          'observation_covariance', 'initial_state_mean', 'initial_state_covariance']\n",
    "        for param in need_params:\n",
    "            setattr(kf, param, getattr(self, param))\n",
    "        _, _ = kf.filter(X)\n",
    "        smoothed_state_means, smoothed_state_covariances = kf.smooth(X)\n",
    "        #################################################################################\n",
    "            \n",
    "        return smoothed_state_means, smoothed_state_covariances  \n",
    "    \n",
    "    def em(self, X, max_iter=10):\n",
    "        \"\"\"\n",
    "        This part is OPTIONAL\n",
    "        Method that perform the EM algorithm to update the model parameters\n",
    "        Note that in this exercise we ignore offsets\n",
    "        @param X: a numpy 2D array whose dimension is [n_example, self.n_dim_obs]\n",
    "        @param max_iter: an integer indicating how many iterations to run\n",
    "        \"\"\"\n",
    "        # validate inputs have right dimensions\n",
    "        n_example, observed_dim = X.shape\n",
    "        assert observed_dim==self.n_dim_obs\n",
    "        \n",
    "        # keep track of log posterior (use function calculate_posterior below)\n",
    "        self.avg_em_log_posterior = np.zeros(max_iter)*np.nan\n",
    "        \n",
    "        #############################\n",
    "        #### TODO: EM iterations ####\n",
    "        #############################\n",
    "        \n",
    "        \n",
    "        \n",
    "                \n",
    "\n",
    "    def import_param(self, kf_model):\n",
    "        \"\"\"\n",
    "        Method that copies parameters from a trained Kalman Model\n",
    "        @param kf_model: a Pykalman object\n",
    "        \"\"\"\n",
    "        need_params = ['transition_matrices', 'observation_matrices', 'transition_covariance', \n",
    "                  'observation_covariance', 'initial_state_mean', 'initial_state_covariance']\n",
    "        for param in need_params:\n",
    "            setattr(self, param, getattr(kf_model, param))\n",
    "            \n",
    "            \n",
    "    def calculate_posterior(self, X, state_mean, v_n=None):\n",
    "        \"\"\"\n",
    "        Method that calculates the log posterior\n",
    "        @param X: a numpy 2D array whose dimension is [n_example, self.n_dim_obs]\n",
    "        @param state_mean: a numpy 2D array whose dimension is [n_example, self.n_dim_state]\n",
    "        @output: a numpy 1D array whose dimension is [n_example]\n",
    "        \"\"\"\n",
    "        \n",
    "        if v_n is None:\n",
    "            _, v_n = self.filter(X)\n",
    "        llh = []\n",
    "        for i in range(1,len(state_mean)):\n",
    "            normal_mean = np.dot(self.observation_matrices, np.dot(self.transition_matrices, state_mean[i-1]))\n",
    "            p_n = self.transition_matrices.dot(v_n[i].dot(self.transition_matrices))+self.transition_covariance\n",
    "            #normal_cov = np.matmul(self.observation_matrices, np.matmul(self.p_n_list[i], self.observation_matrices.T)) + self.observation_covariance\n",
    "            normal_cov = np.matmul(self.observation_matrices, np.matmul(p_n, self.observation_matrices.T)) + self.observation_covariance\n",
    "            pdf_val = multivariate_normal.pdf(X[i], normal_mean, normal_cov)\n",
    "            # replace 0 to prevent numerical underflow\n",
    "            if pdf_val < 1e-10:\n",
    "                pdf_val = 1e-10\n",
    "            llh.append(np.log(pdf_val))            \n",
    "        return np.array(llh)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sampling\n",
    "n_dim_state = 2\n",
    "n_dim_obs = 2\n",
    "kf = KalmanFilter(n_dim_state=n_dim_state, n_dim_obs=n_dim_obs)\n",
    "# set paramters\n",
    "kf.transition_matrices = np.eye(kf.n_dim_state)*.5\n",
    "kf.transition_covariance = np.eye(kf.n_dim_obs)\n",
    "kf.observation_matrices = np.eye(kf.n_dim_state)\n",
    "kf.observation_covariance = np.eye(kf.n_dim_obs)*.1\n",
    "kf.initial_state_mean = np.zeros(kf.n_dim_state)\n",
    "kf.initial_state_covariance = np.eye(kf.n_dim_state)*.1\n",
    "# import to your own kalman object\n",
    "my_kf = MyKalmanFilter(n_dim_state=n_dim_state, n_dim_obs=n_dim_obs)\n",
    "my_kf.import_param(kf)\n",
    "# print the parameters\n",
    "print_parameters(my_kf, evals=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### test that your sampling works:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampled_states, sampled_observations = kf.sample(100, initial_state=kf.initial_state_mean, random_state=np.random.RandomState(0))\n",
    "sampled_states_impl, sampled_observations_impl = my_kf.sample(100, initial_state=kf.initial_state_mean, random_seed=0)\n",
    "print('sampled states pykalman at t=2: ', sampled_states[2,:])\n",
    "print('sampled states own implementation at t=2: ', sampled_states_impl[2,:])\n",
    "fig = plot_kalman(sampled_states_impl[:,0],sampled_states_impl[:,1],sampled_observations_impl[:,0],sampled_observations_impl[:,1], title='sample');\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### reduce observation noise\n",
    "What do you expect should happen?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### TODO ####\n",
    "#### reduce observation noise ####\n",
    "\n",
    "\n",
    "# plot\n",
    "for nn in range(3):\n",
    "    sampled_states_impl, sampled_observations_impl = my_kf.sample(100, initial_state=kf.initial_state_mean, random_seed=nn)\n",
    "    fig = plot_kalman(sampled_states_impl[:,0],sampled_states_impl[:,1],sampled_observations_impl[:,0],sampled_observations_impl[:,1], title='reduced noise sample');\n",
    "    plt.axis('square');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### increase the respective temporal dyamics\n",
    "What do you expect should happen?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### TODO ####\n",
    "#### reduce latent temporal dependency ####\n",
    "\n",
    "\n",
    "# plot\n",
    "for nn in range(3):\n",
    "    sampled_states_impl, sampled_observations_impl = my_kf.sample(100, initial_state=kf.initial_state_mean, random_seed=nn)\n",
    "    fig = plot_kalman(sampled_states_impl[:,0],sampled_states_impl[:,1],\n",
    "                      sampled_observations_impl[:,0],sampled_observations_impl[:,1], title='eval A = '+np.str(.99));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### data to use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kf_GT = KalmanFilter(n_dim_state=n_dim_state, n_dim_obs=n_dim_obs)\n",
    "# set paramters\n",
    "kf_GT.transition_matrices = np.eye(n_dim_state)*.9\n",
    "kf_GT.transition_covariance = np.eye(n_dim_obs)\n",
    "kf_GT.observation_matrices = np.eye(n_dim_state)\n",
    "kf_GT.observation_covariance = np.eye(n_dim_obs)\n",
    "kf_GT.initial_state_mean = np.zeros(n_dim_state)\n",
    "kf_GT.initial_state_covariance = np.eye(n_dim_state)*.1\n",
    "# import to your own kalman object\n",
    "my_kf_GT = MyKalmanFilter(n_dim_state=n_dim_state, n_dim_obs=n_dim_obs)\n",
    "my_kf_GT.import_param(kf_GT)\n",
    "# print the parameters\n",
    "print_parameters(my_kf_GT, evals=True)\n",
    "\n",
    "\n",
    "# sample\n",
    "latent, data = kf_GT.sample(100, initial_state=kf_GT.initial_state_mean, random_state=np.random.RandomState(2))\n",
    "_, _ = kf_GT.filter(data)\n",
    "estlat,_ = kf_GT.smooth(data)\n",
    "fig = plot_kalman(latent[:,0],latent[:,1],data[:,0],data[:,1], title='sample for EM');\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filtering\n",
    "with known parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_state_means_impl, filtered_state_covariances_impl = my_kf_GT.filter(data)\n",
    "fig = plot_kalman(latent[:,0],latent[:,1],data[:,0],data[:,1], filtered_state_means_impl[:,0], filtered_state_means_impl[:,1], \"r-\", title =\"kf-impl-filter\")\n",
    "plt.axis('square');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Smoothing\n",
    "with known parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "smoothed_state_means_impl, smoothed_state_covariances_impl = my_kf_GT.smooth(data)\n",
    "fig = plot_kalman(latent[:,0],latent[:,1],data[:,0],data[:,1], \n",
    "                  smoothed_state_means_impl[:,0], smoothed_state_means_impl[:,1], \"r-\", title=\"kf-impl-smooth\")\n",
    "plt.axis('square');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### run EM\n",
    "to learn parameters (M-step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "\n",
    "iters = 5\n",
    "# perturb starting parameters\n",
    "kf = KalmanFilter(n_dim_state=data.shape[1], n_dim_obs=data.shape[1],\n",
    "                 transition_matrices= np.eye(data.shape[1]),\n",
    "                 observation_matrices= np.eye(data.shape[1])+np.random.randn(data.shape[1])*.1,\n",
    "                 transition_covariance= np.eye(data.shape[1]),\n",
    "                 observation_covariance = np.eye(data.shape[1]),\n",
    "                 initial_state_mean=np.random.randn(data.shape[1]),\n",
    "                  initial_state_covariance = np.eye(data.shape[1]))\n",
    "\n",
    "my_kf = MyKalmanFilter(n_dim_state=data.shape[1], n_dim_obs=data.shape[1])\n",
    "my_kf.import_param(kf)\n",
    "\n",
    "kf.em(data, n_iter=iters)\n",
    "my_kf.em(data, max_iter=iters)\n",
    "\n",
    "print('           pykalman EM:')\n",
    "print(' ')\n",
    "print_parameters(kf, evals=True)\n",
    "print('           own implementation EM:')\n",
    "print(' ')\n",
    "print_parameters(my_kf, evals=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compare the filter results\n",
    "filtered_state_means, filtered_state_covariances = kf.filter(data)\n",
    "filtered_state_means_impl, filtered_state_covariances_impl = my_kf.filter(data)\n",
    "_ = plot_kalman(latent[:,0],latent[:,1],data[:,0],data[:,1], \n",
    "                filtered_state_means[:,0], filtered_state_means[:,1], \"r-\", title=\"pykalman kf-filter\")\n",
    "plt.axis('square');\n",
    "_ = plot_kalman(latent[:,0],latent[:,1],data[:,0],data[:,1], \n",
    "                filtered_state_means_impl[:,0], filtered_state_means_impl[:,1], \"r-\", title=\"own kf-impl-filter\")\n",
    "plt.axis('square');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compare the smooth results\n",
    "smoothed_state_means, smoothed_state_covariances = kf.smooth(data)\n",
    "smoothed_state_means_impl, smoothed_state_covariances_impl = my_kf.smooth(data)\n",
    "_ = plot_kalman(latent[:,0],latent[:,1],data[:,0],data[:,1], \n",
    "                smoothed_state_means[:,0], smoothed_state_means[:,1], \"r-\", title=\"pykalman kf-smoothed\")\n",
    "plt.axis('square');\n",
    "_ = plot_kalman(latent[:,0],latent[:,1],data[:,0],data[:,1], \n",
    "                smoothed_state_means_impl[:,0], smoothed_state_means_impl[:,1], \"r-\", \"own kf-impl-smoothed\")\n",
    "plt.axis('square');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize the change of avg log posterior\n",
    "visualize_line_plot(my_kf.avg_em_log_posterior, \"# iter\", \"avg log posterior\", \"Log Posterior Progress\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Please turn in the code as a notebook AND as a pdf before 09/10/2019 3:00 pm. Please name your notebook netid.ipynb.\n",
    "\n",
    "### Your work will be evaluated based on the code and plots. You don't need to write down your answers to these questions in the text blocks. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
